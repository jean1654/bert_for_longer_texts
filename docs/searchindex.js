Search.setIndex({"docnames": ["belt_nlp", "bert_for_longer_texts", "index", "modules"], "filenames": ["belt_nlp.rst", "bert_for_longer_texts.md", "index.rst", "modules.rst"], "titles": ["belt_nlp package", "BERT modification for longer texts", "Welcome to BELT (BERT For Longer Texts)\u2019s documentation!", "belt_nlp"], "terms": {"On": [], "top": [], "project": 2, "addit": [1, 2], "doc": [], "pip": 2, "r": [], "sphinx": [], "requir": 2, "txt": 2, "gener": [], "build": 2, "start": [], "simpl": [], "http": [0, 2], "server": [], "python": 2, "m": [], "directori": [], "open": [], "0": 0, "8000": [], "make": [0, 1, 2], "sure": 0, "you": 2, "ar": [0, 1, 2], "gh": [], "page": 2, "branch": [], "add": [0, 1, 2], "commit": [], "push": [], "chang": [1, 2], "class": [0, 1, 2], "bertclassifi": [0, 3], "batch_siz": [0, 1, 2], "int": 0, "learning_r": 0, "float": 0, "epoch": 0, "token": [0, 1, 2], "pretrainedtokenizerbas": 0, "none": [0, 1, 2], "neural_network": 0, "pretrained_model_name_or_path": 0, "str": 0, "base": [0, 1, 2], "uncas": 0, "devic": 0, "cuda": [0, 2], "many_gpu": 0, "bool": 0, "fals": 0, "abc": 0, "The": [0, 1, 2], "paramet": [0, 1, 2], "can": [0, 1, 2], "have": 0, "follow": [0, 1, 2], "valu": 0, "cpu": [0, 2], "model": 0, "load": [0, 3], "singl": 0, "gpu": [0, 1, 2], "i": [0, 1, 2], "specif": 0, "index": [0, 2], "It": [0, 1, 2], "also": 0, "possibl": 0, "us": [0, 1, 2], "multipl": 0, "In": [0, 1, 2], "order": [0, 2], "do": [0, 1, 2], "thi": [0, 1, 2], "set": [0, 1, 2], "flag": 0, "true": 0, "As": 0, "default": [0, 1, 2], "all": [0, 1, 2], "them": 0, "To": 0, "onli": [0, 1, 2], "select": 0, "environment": 0, "variabl": [0, 1, 2], "cuda_visible_devic": 0, "fit": [0, 1, 2, 3], "x_train": 0, "list": [0, 1, 2], "y_train": 0, "classmethod": 0, "model_dir": 0, "predict": [0, 3], "x": 0, "tupl": 0, "predict_class": [0, 3], "predict_scor": [0, 3], "save": [0, 3], "bertclassifiernn": [0, 3], "bertmodel": 0, "robertamodel": 0, "forward": [0, 3], "input_id": 0, "tensor": [0, 1, 2], "attention_mask": 0, "defin": 0, "comput": 0, "perform": 0, "everi": 0, "call": 0, "should": 0, "overridden": 0, "subclass": 0, "although": 0, "recip": 0, "pass": 0, "need": [0, 1, 2], "within": 0, "function": [0, 1, 2], "one": [0, 1, 2], "instanc": 0, "afterward": 0, "instead": 0, "sinc": 0, "former": 0, "take": 0, "care": 0, "run": [0, 2], "regist": 0, "hook": 0, "while": 0, "latter": 0, "silent": 0, "ignor": [0, 1, 2], "tokenizeddataset": [0, 3], "batchencod": 0, "label": 0, "dataset": 0, "option": [0, 2], "bertclassifiertrunc": [0, 3], "bertclassifierwithpool": [0, 1, 2, 3], "chunk_siz": [0, 1, 2], "stride": [0, 1, 2], "minimal_chunk_length": [0, 1, 2], "pooling_strategi": [0, 1, 2], "mean": [0, 1, 2], "maximal_text_length": [0, 1, 2], "procedur": [0, 1, 2], "whole": [0, 1, 2], "text": 0, "truncat": [0, 1, 2], "size": [0, 1, 2], "chunk": [0, 1, 2], "mai": [0, 1, 2], "overlap": [0, 1, 2], "depend": [0, 1, 2], "other": [0, 1, 2], "word": [0, 1, 2], "we": [0, 1, 2], "get": [0, 1, 2], "move": [0, 1, 2], "window": [0, 1, 2], "length": [0, 1, 2], "equal": [0, 1, 2], "see": [0, 1, 2], "exampl": [0, 1, 2], "github": 0, "com": 0, "googl": 0, "research": 0, "issu": [0, 1, 2], "27": 0, "issuecom": 0, "435265194": 0, "ha": [0, 1, 2], "analog": [0, 1, 2], "here": [0, 1, 2], "convolut": [0, 1, 2], "neural": [0, 1, 2], "network": [0, 1, 2], "kernel_s": [0, 1, 2], "cnn": [0, 1, 2], "which": [0, 1, 2], "too": [0, 1, 2], "small": [0, 1, 2], "smaller": [0, 1, 2], "than": [0, 1, 2], "after": [0, 1, 2], "pool": [0, 1, 2], "aggreg": [0, 1, 2], "specifi": 0, "string": [0, 1, 2], "either": [0, 1, 2], "max": [0, 1, 2], "static": 0, "collate_fn_pooled_token": [0, 3], "data": 0, "inconsistentsplittingparamsexcept": [0, 3], "add_padding_token": [0, 3], "input_id_chunk": 0, "torch": [0, 1, 2], "mask_chunk": 0, "pad": [0, 1, 2], "id": 0, "end": [0, 1, 2], "exactli": 0, "512": [0, 1, 2], "add_special_tokens_at_beginning_and_end": [0, 3], "special": [0, 1, 2], "cl": 0, "101": 0, "begin": [0, 1, 2], "sep": 0, "102": 0, "each": [0, 1, 2], "correspond": 0, "attent": 0, "mask": 0, "1": 0, "boolean": 0, "check_split_parameters_consist": [0, 3], "split_overlap": [0, 3], "helper": 0, "divid": 0, "dimension": 0, "split_tokens_into_smaller_chunk": [0, 3], "given": [0, 1, 2], "stack_tokens_from_all_chunk": [0, 3], "reshap": 0, "form": 0, "compat": [0, 2], "input": [0, 1, 2], "tokenize_text_with_trunc": [0, 3], "without": 0, "tokenize_whole_text": [0, 3], "entir": 0, "transform_list_of_text": [0, 3], "transform_single_text": [0, 3], "transform": [0, 1, 2], "list_of_tensors_deep_equ": [0, 3], "list_1": 0, "list_2": 0, "maxim": [1, 2], "roughli": [1, 2], "speak": [1, 2], "built": [1, 2], "architectur": [1, 2], "cannot": [1, 2], "directli": [1, 2], "discuss": [1, 2], "found": [1, 2], "overcom": [1, 2], "wa": [1, 2], "propos": [1, 2], "devlin": [1, 2], "author": [1, 2], "previous": [1, 2], "mention": [1, 2], "comment": [1, 2], "split": [1, 2, 3], "determin": [1, 2], "hyperparamet": [1, 2], "These": [1, 2], "thei": [1, 2], "wai": [1, 2], "an": [1, 2], "integ": [1, 2], "number": [1, 2], "consid": [1, 2], "standard": [1, 2], "510": [1, 2], "becaus": [1, 2], "larger": [1, 2], "otherwis": [1, 2], "abl": [1, 2], "bigger": [1, 2], "must": [1, 2], "1d": [1, 2], "aforement": [1, 2], "more": [1, 2], "differ": [1, 2], "test_split": [1, 2], "result": [1, 2], "instruct": [1, 2], "main": [1, 2], "allow": [1, 2], "For": 1, "sequenc": [1, 2], "same": [1, 2], "stack": [1, 2], "via": [1, 2], "fed": [1, 2], "mini": [1, 2], "batch": [1, 2], "n": [1, 2], "probabl": [1, 2], "obtain": [1, 2], "final": [1, 2], "maximum": [1, 2], "dure": [1, 2], "train": [1, 2], "step": [1, 2], "abov": [1, 2], "crucial": [1, 2], "part": [1, 2], "oper": [1, 2], "type": [1, 2], "cat": [1, 2], "done": [1, 2], "attach": [1, 2], "gradient": [1, 2], "That": [1, 2], "ani": [1, 2], "intermedi": [1, 2], "convers": [1, 2], "arrai": [1, 2], "backpropag": [1, 2], "command": [1, 2], "loss": [1, 2], "backward": [1, 2], "won": [1, 2], "t": [1, 2], "work": [1, 2], "precis": [1, 2], "overrid": [1, 2], "loop": [1, 2], "_evaluate_single_batch": [1, 2], "bert_with_pool": [1, 2, 3], "py": [1, 2], "dataload": [1, 2], "automat": [1, 2], "want": [1, 2], "why": [1, 2], "creat": [1, 2], "custom": [1, 2], "overwritten": [1, 2], "collate_fn": [1, 2], "detail": [1, 2], "lot": [1, 2], "memori": [1, 2], "even": [1, 2], "case": [1, 2], "recommend": [1, 2], "natur": [1, 2], "trade": [1, 2], "off": [1, 2], "between": [1, 2], "context": [1, 2], "look": [1, 2], "avail": [1, 2], "resourc": [1, 2], "equival": [1, 2], "belt_nlp": 2, "packag": [2, 3], "search": 2, "submodul": 3, "bert": 3, "modul": 3, "bert_trunc": 3, "except": 3, "tensor_util": 3, "content": 3, "9": 2, "henc": 2, "necessari": 2, "instal": 2, "version": 2, "machin": 2, "driver": 2, "first": 2, "check": 2, "nvidia": 2, "smi": 2, "choos": 2, "newest": 2, "accord": 2, "e": 2, "g": 2, "11": 2, "Then": 2, "find": 2, "our": 2, "anoth": 2, "pip3": 2, "torchvis": 2, "torchaudio": 2, "url": 2, "download": 2, "pytorch": 2, "org": 2, "whl": 2, "next": 2, "nlp": 2, "If": 2, "clone": 2, "repo": 2, "test": 2, "notebook": 2, "file": 2, "accumulation_step": 0}, "objects": {"": [[0, 0, 0, "-", "belt_nlp"]], "belt_nlp": [[0, 0, 0, "-", "bert"], [0, 0, 0, "-", "bert_truncated"], [0, 0, 0, "-", "bert_with_pooling"], [0, 0, 0, "-", "exceptions"], [0, 0, 0, "-", "splitting"], [0, 0, 0, "-", "tensor_utils"]], "belt_nlp.bert": [[0, 1, 1, "", "BertClassifier"], [0, 1, 1, "", "BertClassifierNN"], [0, 1, 1, "", "TokenizedDataset"]], "belt_nlp.bert.BertClassifier": [[0, 2, 1, "", "fit"], [0, 2, 1, "", "load"], [0, 2, 1, "", "predict"], [0, 2, 1, "", "predict_classes"], [0, 2, 1, "", "predict_scores"], [0, 2, 1, "", "save"]], "belt_nlp.bert.BertClassifierNN": [[0, 2, 1, "", "forward"]], "belt_nlp.bert_truncated": [[0, 1, 1, "", "BertClassifierTruncated"]], "belt_nlp.bert_with_pooling": [[0, 1, 1, "", "BertClassifierWithPooling"]], "belt_nlp.bert_with_pooling.BertClassifierWithPooling": [[0, 2, 1, "", "collate_fn_pooled_tokens"]], "belt_nlp.exceptions": [[0, 3, 1, "", "InconsistentSplittingParamsException"]], "belt_nlp.splitting": [[0, 4, 1, "", "add_padding_tokens"], [0, 4, 1, "", "add_special_tokens_at_beginning_and_end"], [0, 4, 1, "", "check_split_parameters_consistency"], [0, 4, 1, "", "split_overlapping"], [0, 4, 1, "", "split_tokens_into_smaller_chunks"], [0, 4, 1, "", "stack_tokens_from_all_chunks"], [0, 4, 1, "", "tokenize_text_with_truncation"], [0, 4, 1, "", "tokenize_whole_text"], [0, 4, 1, "", "transform_list_of_texts"], [0, 4, 1, "", "transform_single_text"]], "belt_nlp.tensor_utils": [[0, 4, 1, "", "list_of_tensors_deep_equal"]]}, "objtypes": {"0": "py:module", "1": "py:class", "2": "py:method", "3": "py:exception", "4": "py:function"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "exception", "Python exception"], "4": ["py", "function", "Python function"]}, "titleterms": {"instal": [], "depend": [], "test": [], "local": [], "publish": [], "belt_nlp": [0, 3], "packag": 0, "submodul": 0, "bert": [0, 1, 2], "modul": 0, "bert_trunc": 0, "bert_with_pool": 0, "except": 0, "split": 0, "tensor_util": 0, "content": [0, 2], "modif": [1, 2], "longer": [1, 2], "text": [1, 2], "motiv": [1, 2], "method": [1, 2], "1": [1, 2], "prepar": [1, 2], "singl": [1, 2], "2": [1, 2], "model": [1, 2], "evalu": [1, 2], "3": [1, 2], "fine": [1, 2], "tune": [1, 2], "classifi": [1, 2], "remark": [1, 2], "welcom": 2, "belt": 2, "For": 2, "": 2, "document": 2, "indic": 2, "tabl": 2}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 8, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx": 57}, "alltitles": {"belt_nlp package": [[0, "belt-nlp-package"]], "Submodules": [[0, "submodules"]], "belt_nlp.bert module": [[0, "module-belt_nlp.bert"]], "belt_nlp.bert_truncated module": [[0, "module-belt_nlp.bert_truncated"]], "belt_nlp.bert_with_pooling module": [[0, "module-belt_nlp.bert_with_pooling"]], "belt_nlp.exceptions module": [[0, "module-belt_nlp.exceptions"]], "belt_nlp.splitting module": [[0, "module-belt_nlp.splitting"]], "belt_nlp.tensor_utils module": [[0, "module-belt_nlp.tensor_utils"]], "Module contents": [[0, "module-belt_nlp"]], "BERT modification for longer texts": [[1, "bert-modification-for-longer-texts"], [2, "bert-modification-for-longer-texts"]], "Motivation": [[1, "motivation"], [2, "motivation"]], "Method": [[1, "method"], [2, "method"]], "1. Preparing a single text": [[1, "preparing-a-single-text"], [2, "preparing-a-single-text"]], "2. Model evaluation": [[1, "model-evaluation"], [2, "model-evaluation"]], "3. Fine-tuning the classifier": [[1, "fine-tuning-the-classifier"], [2, "fine-tuning-the-classifier"]], "Remarks": [[1, "remarks"], [2, "remarks"]], "Welcome to BELT (BERT For Longer Texts)\u2019s documentation!": [[2, "welcome-to-belt-bert-for-longer-texts-s-documentation"]], "Contents:": [[2, null]], "Indices and tables": [[2, "indices-and-tables"]], "belt_nlp": [[3, "belt-nlp"]]}, "indexentries": {"bertclassifier (class in belt_nlp.bert)": [[0, "belt_nlp.bert.BertClassifier"]], "bertclassifiernn (class in belt_nlp.bert)": [[0, "belt_nlp.bert.BertClassifierNN"]], "bertclassifiertruncated (class in belt_nlp.bert_truncated)": [[0, "belt_nlp.bert_truncated.BertClassifierTruncated"]], "bertclassifierwithpooling (class in belt_nlp.bert_with_pooling)": [[0, "belt_nlp.bert_with_pooling.BertClassifierWithPooling"]], "inconsistentsplittingparamsexception": [[0, "belt_nlp.exceptions.InconsistentSplittingParamsException"]], "tokenizeddataset (class in belt_nlp.bert)": [[0, "belt_nlp.bert.TokenizedDataset"]], "add_padding_tokens() (in module belt_nlp.splitting)": [[0, "belt_nlp.splitting.add_padding_tokens"]], "add_special_tokens_at_beginning_and_end() (in module belt_nlp.splitting)": [[0, "belt_nlp.splitting.add_special_tokens_at_beginning_and_end"]], "belt_nlp": [[0, "module-belt_nlp"]], "belt_nlp.bert": [[0, "module-belt_nlp.bert"]], "belt_nlp.bert_truncated": [[0, "module-belt_nlp.bert_truncated"]], "belt_nlp.bert_with_pooling": [[0, "module-belt_nlp.bert_with_pooling"]], "belt_nlp.exceptions": [[0, "module-belt_nlp.exceptions"]], "belt_nlp.splitting": [[0, "module-belt_nlp.splitting"]], "belt_nlp.tensor_utils": [[0, "module-belt_nlp.tensor_utils"]], "check_split_parameters_consistency() (in module belt_nlp.splitting)": [[0, "belt_nlp.splitting.check_split_parameters_consistency"]], "collate_fn_pooled_tokens() (belt_nlp.bert_with_pooling.bertclassifierwithpooling static method)": [[0, "belt_nlp.bert_with_pooling.BertClassifierWithPooling.collate_fn_pooled_tokens"]], "fit() (belt_nlp.bert.bertclassifier method)": [[0, "belt_nlp.bert.BertClassifier.fit"]], "forward() (belt_nlp.bert.bertclassifiernn method)": [[0, "belt_nlp.bert.BertClassifierNN.forward"]], "list_of_tensors_deep_equal() (in module belt_nlp.tensor_utils)": [[0, "belt_nlp.tensor_utils.list_of_tensors_deep_equal"]], "load() (belt_nlp.bert.bertclassifier class method)": [[0, "belt_nlp.bert.BertClassifier.load"]], "module": [[0, "module-belt_nlp"], [0, "module-belt_nlp.bert"], [0, "module-belt_nlp.bert_truncated"], [0, "module-belt_nlp.bert_with_pooling"], [0, "module-belt_nlp.exceptions"], [0, "module-belt_nlp.splitting"], [0, "module-belt_nlp.tensor_utils"]], "predict() (belt_nlp.bert.bertclassifier method)": [[0, "belt_nlp.bert.BertClassifier.predict"]], "predict_classes() (belt_nlp.bert.bertclassifier method)": [[0, "belt_nlp.bert.BertClassifier.predict_classes"]], "predict_scores() (belt_nlp.bert.bertclassifier method)": [[0, "belt_nlp.bert.BertClassifier.predict_scores"]], "save() (belt_nlp.bert.bertclassifier method)": [[0, "belt_nlp.bert.BertClassifier.save"]], "split_overlapping() (in module belt_nlp.splitting)": [[0, "belt_nlp.splitting.split_overlapping"]], "split_tokens_into_smaller_chunks() (in module belt_nlp.splitting)": [[0, "belt_nlp.splitting.split_tokens_into_smaller_chunks"]], "stack_tokens_from_all_chunks() (in module belt_nlp.splitting)": [[0, "belt_nlp.splitting.stack_tokens_from_all_chunks"]], "tokenize_text_with_truncation() (in module belt_nlp.splitting)": [[0, "belt_nlp.splitting.tokenize_text_with_truncation"]], "tokenize_whole_text() (in module belt_nlp.splitting)": [[0, "belt_nlp.splitting.tokenize_whole_text"]], "transform_list_of_texts() (in module belt_nlp.splitting)": [[0, "belt_nlp.splitting.transform_list_of_texts"]], "transform_single_text() (in module belt_nlp.splitting)": [[0, "belt_nlp.splitting.transform_single_text"]]}})