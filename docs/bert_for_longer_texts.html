<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>BERT modification for longer texts &mdash; BELT (BERT For Longer Texts) 2021 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            BELT (BERT For Longer Texts)
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="modules.html">belt_nlp</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">BELT (BERT For Longer Texts)</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">BERT modification for longer texts</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/bert_for_longer_texts.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="bert-modification-for-longer-texts">
<h1>BERT modification for longer texts<a class="headerlink" href="#bert-modification-for-longer-texts" title="Permalink to this heading"></a></h1>
<section id="motivation">
<h2>Motivation<a class="headerlink" href="#motivation" title="Permalink to this heading"></a></h2>
<p>The BERT model can only use the text of the maximal length of 512 tokens (roughly speaking: token = word). It is built in the model architecture and cannot be directly changed. Discussion of this issue can be found <a class="reference external" href="https://github.com/google-research/bert/issues/27">here</a>.</p>
</section>
<section id="method">
<h2>Method<a class="headerlink" href="#method" title="Permalink to this heading"></a></h2>
<p>Method to overcome this issue was proposed by Devlin (one of the authors of BERT) in the previously mentioned discussion: <a class="reference external" href="https://github.com/google-research/bert/issues/27#issuecomment-435265194">comment</a>.</p>
<p>The procedure of splitting and pooling is determined by the hyperparameters of the class <code class="docutils literal notranslate"><span class="pre">BertClassifierWithPooling</span></code>. These are <code class="docutils literal notranslate"><span class="pre">maximal_text_length</span></code>, <code class="docutils literal notranslate"><span class="pre">chunk_size</span></code>, <code class="docutils literal notranslate"><span class="pre">stride</span></code>, <code class="docutils literal notranslate"><span class="pre">minimal_chunk_length</span></code>,  and <code class="docutils literal notranslate"><span class="pre">pooling_strategy</span></code>.
They are used in the following way:</p>
<ul class="simple">
<li><p>The parameter <code class="docutils literal notranslate"><span class="pre">maximal_text_length</span></code> is used to truncate the tokens. It can be either <code class="docutils literal notranslate"><span class="pre">None</span></code>, which means no truncation, or an integer, determining the number of tokens to consider. Standard BERT truncates to 510 tokens because it needs 2 additional tokens at the beginning and the end.</p></li>
<li><p>The integer parameter <code class="docutils literal notranslate"><span class="pre">chunk_size</span></code> determines the size (in number of tokens) of each chunk. This parameter cannot be larger than 510. Otherwise, we will not be able to fit the chunk into the input of BERT.</p></li>
<li><p>Tokens may overlap depending on the parameter <code class="docutils literal notranslate"><span class="pre">stride</span></code>.</p></li>
<li><p>In other words: we get chunks by moving the window of the size <code class="docutils literal notranslate"><span class="pre">chunk_size</span></code> by the length equal to <code class="docutils literal notranslate"><span class="pre">stride</span></code>. Stride cannot be bigger than chunk size. Chunks must overlap or be near each other.</p></li>
<li><p>Stride has the analogous meaning here to that in <a class="reference external" href="https://deepai.org/machine-learning-glossary-and-terms/stride">convolutional neural networks</a>.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">chunk_size</span></code> is analogous to <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html">kernel_size</a> in 1D CNNs.</p></li>
<li><p>We ignore chunks which are too small - smaller than <code class="docutils literal notranslate"><span class="pre">minimal_chunk_length</span></code>. This parameter cannot be set larger than <code class="docutils literal notranslate"><span class="pre">chunk_size</span></code>.</p></li>
<li><p>See the example in <a class="reference external" href="https://github.com/google-research/bert/issues/27#issuecomment-435265194">the aforementioned comment</a>.</p></li>
<li><p>More examples of splitting with different sets of parameters are in <a class="reference external" href="https://github.com/mim-solutions/bert_for_longer_texts/blob/main/tests/test_splitting.py">test_splitting</a>.</p></li>
<li><p>The string parameter <code class="docutils literal notranslate"><span class="pre">pooling_strategy</span></code> is used at the end to aggregate the model results. It can be either <code class="docutils literal notranslate"><span class="pre">mean</span></code> or <code class="docutils literal notranslate"><span class="pre">max</span></code>.</p></li>
</ul>
<section id="preparing-a-single-text">
<h3>1. Preparing a single text<a class="headerlink" href="#preparing-a-single-text" title="Permalink to this heading"></a></h3>
<p>We follow <a class="reference external" href="https://www.kdnuggets.com/2021/04/apply-transformers-any-length-text.html">this instruction</a>. The main difference is that we allow the text chunks to overlap.</p>
<ul class="simple">
<li><p>Tokenize the whole text (if <code class="docutils literal notranslate"><span class="pre">maximal_text_length=None</span></code>) or truncate to the size <code class="docutils literal notranslate"><span class="pre">maximal_text_length</span></code>.</p></li>
<li><p>Split the tokens into chunks based on the model hyperparameters <code class="docutils literal notranslate"><span class="pre">chunk_size</span></code>, <code class="docutils literal notranslate"><span class="pre">stride</span></code>, and <code class="docutils literal notranslate"><span class="pre">minimal_chunk_length</span></code>.</p></li>
<li><p>For each chunk add special tokens at the beginning and the end.</p></li>
<li><p>Add padding tokens to make all tokenized sequences the same length.</p></li>
<li><p>Stack the tensor chunks into one via <code class="docutils literal notranslate"><span class="pre">torch.stack</span></code>.</p></li>
</ul>
</section>
<section id="model-evaluation">
<h3>2. Model evaluation<a class="headerlink" href="#model-evaluation" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>The stacked tensor is then fed into the model as a mini-batch.</p></li>
<li><p>We get $N$ probabilities, one for each text chunk.</p></li>
<li><p>We obtain the final probability by using the aggregation function on these probabilities (this function is mean or maximum - it depends on the hyperparameter <code class="docutils literal notranslate"><span class="pre">pooling_strategy</span></code>).</p></li>
</ul>
</section>
<section id="fine-tuning-the-classifier">
<h3>3. Fine-tuning the classifier<a class="headerlink" href="#fine-tuning-the-classifier" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>During training, we do the same steps as above. The crucial part is that all the operations of the type <code class="docutils literal notranslate"><span class="pre">cat/stack/split/mean/max</span></code> must be done on tensors with the attached gradient. That is, we use built-in torch tensor transformations. Any intermediate conversions to lists or arrays are not allowed. Otherwise, the crucial backpropagation command <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> won’t work. More precisely, we override the standard <code class="docutils literal notranslate"><span class="pre">torch</span></code> training loop in the method <code class="docutils literal notranslate"><span class="pre">_evaluate_single_batch</span></code> in the <a class="reference external" href="https://github.com/mim-solutions/bert_for_longer_texts/blob/main/belt_nlp/bert_with_pooling.py">bert_with_pooling.py</a>.</p></li>
<li><p>Because the number of chunks for the given input text is variable, texts after tokenization are tensors with variable length. The default torch class <code class="docutils literal notranslate"><span class="pre">Dataloader</span></code> cannot allow this (because it automatically wants to stack the tensors). That is why we create custom dataloaders with overwritten method <code class="docutils literal notranslate"><span class="pre">collate_fn</span></code> - more details can be found <a class="reference external" href="https://discuss.pytorch.org/t/dataloader-for-various-length-of-data/6418">here</a>.</p></li>
</ul>
</section>
</section>
<section id="remarks">
<h2>Remarks<a class="headerlink" href="#remarks" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p>Because we fed all the text chunks as a mini-batch, the procedure may use a lot of GPU memory to fit all the gradients during fine-tuning even with <code class="docutils literal notranslate"><span class="pre">batch_size=1</span></code>. In this case, we recommend setting the parameter <code class="docutils literal notranslate"><span class="pre">maximal_text_length</span></code> to truncate longer texts. Naturally, this is the trade-off between the context we want the model to look at and the available resources. Setting <code class="docutils literal notranslate"><span class="pre">maximal_text_length=510</span></code> is equivalent to using the standard BERT model with truncation.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, MIM Solutions.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>